---
layout: post
title: Evaluation
---
look @ ieee vis and CHI papers for evaluation

[Knowledge Precepts for Design and Evaluation of Information Visualizations (Amar & Stasko, 2005)](https://www.cc.gatech.edu/~john.stasko/papers/tvcg05.pdf) looks at how limitations in information visualization systems result in analytic gaps between the systems and higher-level analysis tasks such as learning and decision-making; more specifically: the _Worldview Gap_ (what is shown vs. what should actually be shown) and the _Rationale Gap_ (perceiving a relatioinship vs. understanding the usefulness of the relationship). The paper suggests three ways to narrow each gap.

[Strategies for Evaluating Information Visualization Tools:  
Multi-dimensional In-depth Long-term Case Studies (Shneiderman & Plaisant 2006)](https://www.cs.umd.edu/users/ben/papers/Shneiderman2006Strategies.pdf) discusses using multi-dimensional (observations, surveys, etc) in-depth (engagement with users) long-term (from training to expert usage) case studies **(MILCs)** in order to better understand the user. They discuss guidelines for applied ethnographic methods (Section 6) that includes how to prepare, things to do, and how to evaluate current designs. 

[A Nested Model for Visualization Design and Validation (Munzner 2009)](http://www.cs.ubc.ca/labs/imager/tr/2009/NestedModel/NestedModel.pdf) proposes splitting visualization design into a four layer model to analyse and guide design processes. Each layer affects the successive one, and breaking the process down into these layers allows designers to explicitly pinpoint threats and validations (what can go wrong and how problems can be averted). Miscommunication can be avoided by explicitly stating threats to downstream levels and assumptions about upstream levels.

This [Storytelling with Data post](http://www.storytellingwithdata.com/blog/2018/10/10/three-tips-for-storytelling-with-qualitative-data) walks through evaluating and redesigning a visualization of qualitative data, which was, in this case, customer concerns expressed on a survey. The post expands on the three tips and gives a thorough explanation of the steps and choices taken in the redesign. In a similar vein, [this post](https://www.storytellingwithdata.com/blog/2012/08/evaluating-word-clouds) tackles evaluating word clouds (when they're useful and when they're not).

[Empirical Studies in Information Visualization: Seven Scenarios (Lam et. al 2012)](https://hal.inria.fr/file/index/docid/932606/filename/Lam_2012_ESI.pdf) considers how information visualization is evaluated in seven different scenarios and includes contemporary practices and different design approaches. They conducted a literature review of evaluation methods used in ~360 published papers and created the scenarios from their findings. Each scenario includes questions to ask and different ways of interacting with the user.

## Misc / Resources
[VizItCards: A Card-Based Toolkit for Infovis Design Education (He & Adar: IEEE Vis 2016)](http://cond.org/vizitcards.pdf) discusses a card-based workshop to be used in teaching infovis. They specific the learning goals they consider, as well as which aspects of design that the cards can be used to support. 

[Creative User-Centered Visualization Design for Energy Analysts and Modelers](http://openaccess.city.ac.uk/2618/5/Goodwin-preprint.pdf) discusses techniques that deliberately promote creativity in contexts of design problems where the data is relatively unknown and the needs are not well understood.